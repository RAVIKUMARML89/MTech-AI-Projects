{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5407,
          "databundleVersionId": 868283,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "House Prices Prediction: Multiple Models ",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# House Prices - Advanced Regression Technique\n",
        "Predict sales prices using XGBoost, RandomForest, CatBoost, ElasticNet, and ensemble models"
      ],
      "metadata": {
        "id": "rw0is6iFboPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initialization"
      ],
      "metadata": {
        "id": "X5hpx09MboPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the libraries"
      ],
      "metadata": {
        "id": "QeR2NeNkboPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.base import RegressorMixin, BaseEstimator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import optuna\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from typing import Tuple\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.ensemble import VotingRegressor"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "CdBvHD16boPH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the dataset"
      ],
      "metadata": {
        "id": "xEEhsvvkboPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "23CIpEkPboPJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define helper functions"
      ],
      "metadata": {
        "id": "IMCLTk-LboPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cb_objective(\n",
        "    trial: optuna.trial.Trial,\n",
        "    train_pool: Pool,\n",
        "    val_pool: Pool\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Optuna objective for CatBoostRegressor, minimizing RMSE on the validation set.\n",
        "    \"\"\"\n",
        "    param = {\n",
        "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True),\n",
        "        \"random_seed\": 42,\n",
        "        \"verbose\": 0,\n",
        "    }\n",
        "    model = CatBoostRegressor(**param)\n",
        "    model.fit(\n",
        "        train_pool,\n",
        "        eval_set=val_pool,\n",
        "        early_stopping_rounds=50,\n",
        "        use_best_model=True\n",
        "    )\n",
        "    preds = model.predict(val_pool)\n",
        "    rmse = mean_squared_error(val_pool.get_label(), preds, squared=False)\n",
        "    return rmse\n",
        "\n",
        "def en_objective(\n",
        "    trial: optuna.trial.Trial,\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    y_val: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Optuna objective for ElasticNet: tune `alpha` and `l1_ratio`, evaluate RMSE.\n",
        "    \"\"\"\n",
        "    # Suggest hyperparameters: alpha ∈ [1e-4, 10] (log scale), l1_ratio ∈ [0.0, 1.0] :contentReference[oaicite:17]{index=17}\n",
        "    param = {\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True),\n",
        "        \"l1_ratio\": trial.suggest_float(\"l1_ratio\", 0.0, 1.0),\n",
        "        \"fit_intercept\": True,\n",
        "        \"max_iter\": 10_000,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "\n",
        "    # Instantiate and fit (escapes any mutation of the outside data) :contentReference[oaicite:18]{index=18}\n",
        "    model = ElasticNet(**param)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on validation set and return RMSE (since RMSE is the chosen metric for submission) :contentReference[oaicite:19]{index=19}\n",
        "    preds = model.predict(X_val)\n",
        "    rmse = mean_squared_error(y_val, preds, squared=False)\n",
        "    return rmse\n",
        "\n",
        "def filter_numerical_by_label_corr(\n",
        "    input_labels: pd.DataFrame,\n",
        "    input_features: pd.DataFrame,\n",
        "    threshold: float = 0.45\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Keep only those columns in `input_features` whose absolute Pearson correlation\n",
        "    with the single column in `input_labels` is ≥ threshold. Prints kept and dropped features.\n",
        "\n",
        "    Parameters:\n",
        "        input_labels (pd.DataFrame): DataFrame with exactly one numeric column (the target).\n",
        "        input_features (pd.DataFrame): DataFrame with multiple numeric feature columns.\n",
        "        threshold (float): Minimum absolute correlation to keep a feature.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of input_features containing only columns with |corr| ≥ threshold.\n",
        "    \"\"\"\n",
        "    # Work on copies so as not to modify inputs\n",
        "    labels_copy = input_labels.copy()\n",
        "    features_copy = input_features.copy()\n",
        "\n",
        "    label_col = labels_copy.columns[0]\n",
        "\n",
        "    # Join copies and compute correlations\n",
        "    df_joined = pd.concat([features_copy, labels_copy], axis=1)\n",
        "    corr_with_label = df_joined.corr()[label_col].abs().drop(label_col)\n",
        "\n",
        "    features_to_keep = corr_with_label[corr_with_label >= threshold].index.tolist()\n",
        "    results = features_copy[features_to_keep].copy()\n",
        "\n",
        "    print(f\"Kept (|corr| ≥ {threshold}): {features_to_keep}\")\n",
        "    dropped = [f for f in features_copy.columns if f not in features_to_keep]\n",
        "    print(f\"Dropped (|corr| < {threshold}): {dropped}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def label_encode_categorical(categorical_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Label-encodes each column in the input DataFrame of categorical features.\n",
        "    Uses pandas categorical codes, mapping unseen or NaN to -1.\n",
        "\n",
        "    Parameters:\n",
        "        categorical_df (pd.DataFrame): DataFrame containing only categorical (object-dtype) columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame where each column is replaced by its .cat.codes.\n",
        "    \"\"\"\n",
        "    encoded_df = categorical_df.copy()\n",
        "    for col in encoded_df.columns:\n",
        "        # Operate on the copy only\n",
        "        encoded_df[col] = encoded_df[col].astype(\"category\").cat.codes\n",
        "    return encoded_df\n",
        "\n",
        "\n",
        "def plot_label_vs_features(\n",
        "    labels: pd.DataFrame,\n",
        "    numerical_features: pd.DataFrame,\n",
        "    cols_per_row: int = 3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot scatter plots of the label against each numerical feature in a grid layout.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    labels : pd.DataFrame\n",
        "        A DataFrame whose first column is the target series.\n",
        "    numerical_features : pd.DataFrame\n",
        "        DataFrame containing only the numerical feature columns to plot against the label.\n",
        "    cols_per_row : int, default=3\n",
        "        Number of scatter plots per row in the grid.\n",
        "    \"\"\"\n",
        "    # Work on copies so as not to modify inputs\n",
        "    labels_copy = labels.copy()\n",
        "    features_copy = numerical_features.copy()\n",
        "\n",
        "    series_labels = labels_copy.iloc[:, 0]\n",
        "    feature_cols = features_copy.columns.tolist()\n",
        "    n_features = len(feature_cols)\n",
        "    n_rows = math.ceil(n_features / cols_per_row)\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        n_rows,\n",
        "        cols_per_row,\n",
        "        figsize=(5 * cols_per_row, 4 * n_rows),\n",
        "        squeeze=False\n",
        "    )\n",
        "    axes_flat = axes.flatten()\n",
        "\n",
        "    for idx, feat in enumerate(feature_cols):\n",
        "        ax = axes_flat[idx]\n",
        "        ax.scatter(\n",
        "            features_copy[feat],\n",
        "            series_labels.values,\n",
        "            alpha=0.7,\n",
        "            s=10\n",
        "        )\n",
        "        ax.set_xlabel(feat)\n",
        "        ax.set_ylabel(series_labels.name or \"label\")\n",
        "        ax.set_title(f\"{series_labels.name or 'Label'} vs. {feat}\")\n",
        "\n",
        "    for empty_idx in range(n_features, n_rows * cols_per_row):\n",
        "        axes_flat[empty_idx].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def rf_objective(\n",
        "    trial: optuna.trial.Trial,\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    X_val: pd.DataFrame,\n",
        "    y_val: pd.Series\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Optuna objective for RandomForestRegressor, minimizing RMSE.\n",
        "    \"\"\"\n",
        "    param = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1\n",
        "    }\n",
        "\n",
        "    # Copy inputs to avoid mutation\n",
        "    X_tr = X_train.copy()\n",
        "    y_tr = y_train.copy()\n",
        "    X_vl = X_val.copy()\n",
        "    y_vl = y_val.copy()\n",
        "\n",
        "    model = RandomForestRegressor(**param)\n",
        "    model.fit(X_tr, y_tr)\n",
        "\n",
        "    preds = model.predict(X_vl)\n",
        "    rmse = mean_squared_error(y_vl, preds, squared=False)\n",
        "    return rmse\n",
        "\n",
        "def split_categorical_numerical(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits a DataFrame into categorical (object dtype) and numerical (float64 or int64) subsets.\n",
        "    Also prints the count of each.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        categorical_df (pd.DataFrame): A copy of all columns with dtype 'object'.\n",
        "        numerical_df (pd.DataFrame): A copy of all columns with dtype 'float64' or 'int64'.\n",
        "    \"\"\"\n",
        "    categorical_df = df.select_dtypes(include=[\"object\"]).copy()\n",
        "    numerical_df = df.select_dtypes(include=[\"float64\", \"int64\"]).copy()\n",
        "\n",
        "    print(\"Categorical features count: \", len(categorical_df.columns))\n",
        "    print(\"Numerical features count: \",   len(numerical_df.columns))\n",
        "\n",
        "    return categorical_df, numerical_df\n",
        "\n",
        "\n",
        "def split_labels_features(df: pd.DataFrame, target_col: str = \"SalePrice\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits a DataFrame into labels (single-column DataFrame) and features (all other columns).\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame containing the target column.\n",
        "        target_col (str): Name of the column to use as labels.\n",
        "\n",
        "    Returns:\n",
        "        labels (pd.DataFrame): A one-column DataFrame containing the target.\n",
        "        features (pd.DataFrame): A copy of the original DataFrame without the target column.\n",
        "    \"\"\"\n",
        "    labels = df[[target_col]].copy()\n",
        "    features = df.drop(target_col, axis=1).copy()\n",
        "    return labels, features\n",
        "\n",
        "\n",
        "def xgb_objective(\n",
        "    trial: optuna.trial.Trial,\n",
        "    features_train: pd.DataFrame,\n",
        "    labels_train: pd.Series,\n",
        "    features_val: pd.DataFrame,\n",
        "    labels_val: pd.Series\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Optuna objective for XGBoost regression.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    trial : optuna.trial.Trial\n",
        "        The trial object for suggesting hyperparameters.\n",
        "    features_train : pd.DataFrame\n",
        "        Training features.\n",
        "    labels_train : pd.Series\n",
        "        Training labels.\n",
        "    features_val : pd.DataFrame\n",
        "        Validation features.\n",
        "    labels_val : pd.Series\n",
        "        Validation labels.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        Validation RMSE to minimize.\n",
        "    \"\"\"\n",
        "    param = {\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"eval_metric\": \"rmse\",\n",
        "        \"seed\": 42,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 3e-1, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "    }\n",
        "\n",
        "    # Work with copies to avoid mutating the caller’s DataFrames\n",
        "    ft_train = features_train.copy()\n",
        "    lb_train = labels_train.copy()\n",
        "    ft_val = features_val.copy()\n",
        "    lb_val = labels_val.copy()\n",
        "\n",
        "    dtrain = xgb.DMatrix(ft_train, label=lb_train, enable_categorical=True)\n",
        "    dval = xgb.DMatrix(ft_val, label=lb_val, enable_categorical=True)\n",
        "\n",
        "    bst = xgb.train(\n",
        "        param,\n",
        "        dtrain,\n",
        "        num_boost_round=1000,\n",
        "        evals=[(dtrain, \"train\"), (dval, \"validation\")],\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=False,\n",
        "    )\n",
        "\n",
        "    preds = bst.predict(dval)\n",
        "    rmse = mean_squared_error(lb_val, preds, squared=False)\n",
        "    return rmse"
      ],
      "metadata": {
        "trusted": true,
        "id": "DkD1zJh_boPJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Analyze the Data"
      ],
      "metadata": {
        "id": "t3sop7nVboPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train dataset shape: {}\".format(df.shape))"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "c9aYSu6UboPK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "qRu6VZhVboPK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Id', axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "NBJ2Db4bboPL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "el6zpww6boPL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate label from the features\n",
        "df_labels, df_feats = split_labels_features(df)\n",
        "\n",
        "print(\"labels shape: \", df_labels.shape)\n",
        "print(\"features shape: \", df_feats.shape)\n",
        "\n",
        "print(df_labels.describe())"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "kbR1xgcpboPL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "col = df_labels['SalePrice']\n",
        "print(\"Labels has NaN? \", col.isna().any())\n",
        "print(\"Labels has has ±inf? \", np.isinf(col).any())"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "vK6KitE-boPL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce the Less Relevant Features\n",
        "Too much features can leads to overfitting, therefore it's a good idea to reduce the number of features where possible."
      ],
      "metadata": {
        "id": "Y-O67NnuboPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the available data types\n",
        "list(set(df_feats.dtypes.tolist()))"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "nolMLX0YboPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Split categorical from numerical features\n",
        "df_feat_cats, df_feat_nums = split_categorical_numerical(df_feats)\n",
        "\n",
        "print(\"Categorical features count: \", len(df_feat_cats.columns))\n",
        "print(\"Numerical features count: \", len(df_feat_nums.columns))"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "iyLYbsLbboPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_label_vs_features(df_labels, df_feat_nums, cols_per_row=4)"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "4hchP3WCboPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove less relevant numerical features\n",
        "df_feat_nums = filter_numerical_by_label_corr(df_labels, df_feat_nums, threshold=0.45)\n",
        "\n",
        "print(df_feat_nums.shape)"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "WQylq9TnboPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_label_vs_features(df_labels, df_feat_nums, cols_per_row=4)"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "hF_vJnhhboPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_feats = pd.concat([df_feat_cats, df_feat_nums], axis=1)\n",
        "\n",
        "# Split training & validation data (80% training, 20% validation)\n",
        "df_feat_trains, df_feat_vals, df_label_trains, df_label_vals = train_test_split(\n",
        "    df_feats, df_labels, test_size=0.20, random_state=42\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "7qW-b30OboPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model: XGBoost\n",
        "\n",
        "XGBoost is a powerful and efficient gradient boosting library that excels in predictive modeling. It's known for its speed, accuracy, and scalability, making it a popular choice for both classification and regression tasks."
      ],
      "metadata": {
        "id": "-tIavJKOboPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare training data"
      ],
      "metadata": {
        "id": "vUk3EgZKboPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make raw copies so we don’t mutate originals\n",
        "xgb_X_train_raw = df_feat_trains.copy()\n",
        "xgb_X_val_raw   = df_feat_vals.copy()\n",
        "\n",
        "# 2. Replace infinities with NaN (XGBoost can handle NaN, but OneHotEncoder and median imputer need finite values)\n",
        "xgb_X_train_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "xgb_X_val_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 3. Identify which columns are numeric vs. categorical\n",
        "xgb_num_cols = xgb_X_train_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "xgb_cat_cols = xgb_X_train_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# 4. Build separate pipelines for numeric and categorical processing\n",
        "#    - Numeric: median imputation → standard scaling (scaling helps optimization, though not strictly required by XGBoost)\n",
        "#    - Categorical: most_frequent imputation → one-hot encoding (ignore unknowns in validation)\n",
        "numeric_pipeline = Pipeline([\n",
        "    (\"impute_num\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    (\"impute_cat\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "# 5. Combine them in a ColumnTransformer\n",
        "xgb_preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_pipeline, xgb_num_cols),\n",
        "    (\"cat\", categorical_pipeline, xgb_cat_cols)\n",
        "], remainder=\"drop\")  # drop any other columns (should be none)\n",
        "\n",
        "# 6. Fit on training data and transform both train & validation\n",
        "xgb_X_train_arr = xgb_preprocessor.fit_transform(xgb_X_train_raw)\n",
        "xgb_X_val_arr   = xgb_preprocessor.transform(xgb_X_val_raw)\n",
        "\n",
        "# 7. Retrieve the feature names in the correct order\n",
        "#    - Numeric names stay as-is\n",
        "#    - OneHotEncoder produces names like \"column_value\"\n",
        "xgb_num_feature_names = xgb_num_cols\n",
        "xgb_cat_feature_names = (\n",
        "    xgb_preprocessor\n",
        "    .named_transformers_[\"cat\"]\n",
        "    .named_steps[\"onehot\"]\n",
        "    .get_feature_names_out(xgb_cat_cols)\n",
        "    .tolist()\n",
        ")\n",
        "\n",
        "xgb_feature_names = xgb_num_feature_names + xgb_cat_feature_names\n",
        "\n",
        "# 8. Build the final DataFrames for XGBoost\n",
        "xgb_features_train = pd.DataFrame(\n",
        "    xgb_X_train_arr,\n",
        "    index=xgb_X_train_raw.index,\n",
        "    columns=xgb_feature_names\n",
        ")\n",
        "\n",
        "xgb_features_val = pd.DataFrame(\n",
        "    xgb_X_val_arr,\n",
        "    index=xgb_X_val_raw.index,\n",
        "    columns=xgb_feature_names\n",
        ")\n",
        "\n",
        "# 9. Copy labels (as a 1D Series) – XGBoost will accept a Series directly\n",
        "xgb_labels_train = df_label_trains.iloc[:, 0].copy()\n",
        "xgb_labels_val   = df_label_vals.iloc[:, 0].copy()"
      ],
      "metadata": {
        "trusted": true,
        "id": "OkB4QtRpboPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters Tuning"
      ],
      "metadata": {
        "id": "8IMAMMhQboPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_study = optuna.create_study(direction=\"minimize\")\n",
        "xgb_study.optimize(\n",
        "    lambda trial: xgb_objective(\n",
        "        trial,\n",
        "        xgb_features_train,\n",
        "        xgb_labels_train,\n",
        "        xgb_features_val,\n",
        "        xgb_labels_val\n",
        "    ),\n",
        "    n_trials=50\n",
        ")\n",
        "\n",
        "# Print best parameters and best value\n",
        "print(\"Best trial:\")\n",
        "print(f\"  Validation RMSE: {xgb_study.best_value:.4f}\")\n",
        "print(\"  Best hyperparameters:\")\n",
        "for key, val in xgb_study.best_params.items():\n",
        "    print(f\"    {key}: {val}\")\n",
        "\n",
        "# Retrain a final model on the full training set using best_params\n",
        "xgb_best_params = xgb_study.best_params.copy()\n",
        "xgb_best_params.update({\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eval_metric\": \"rmse\",\n",
        "    \"seed\": 42,\n",
        "})"
      ],
      "metadata": {
        "trusted": true,
        "id": "b-KzS8gzboPN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model using best hyperparameters"
      ],
      "metadata": {
        "id": "EZT4wNkrboPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_dtrain = xgb.DMatrix(xgb_features_train, label=xgb_labels_train, enable_categorical=True)\n",
        "xgb_dval = xgb.DMatrix(xgb_features_val, label=xgb_labels_val, enable_categorical=True)\n",
        "xgb_evals_result = {}\n",
        "xgb_bst = xgb.train(\n",
        "    xgb_best_params,\n",
        "    xgb_dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(xgb_dtrain, \"train\"), (xgb_dval, \"validation\")],\n",
        "    evals_result=xgb_evals_result,\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=False,\n",
        ")\n",
        "\n",
        "# Calculate the accuracy %\n",
        "xgb_val_preds = xgb_bst.predict(xgb_dval)\n",
        "xgb_val_rmse = mean_squared_error(xgb_labels_val, xgb_val_preds, squared=False)\n",
        "xgb_mape = mean_absolute_percentage_error(xgb_labels_val, xgb_val_preds)\n",
        "xgb_accuracy_pct = (1.0 - xgb_mape) * 100.0\n",
        "print(f\"[Final model] RMSE: {xgb_val_rmse:.4f}\")\n",
        "print(f\"[Final model] Validation accuracy: {xgb_accuracy_pct:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LOM__TY0boPN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "Un-nSXfjboPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Plot training vs. validation RMSE over iterations\n",
        "xgb_train_rmse        = xgb_evals_result[\"train\"][\"rmse\"]\n",
        "xgb_val_rmse_history  = xgb_evals_result[\"validation\"][\"rmse\"]\n",
        "xgb_iterations        = list(range(1, len(xgb_train_rmse) + 1))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(xgb_iterations, xgb_train_rmse,       label=\"Train RMSE\",      color=\"blue\")\n",
        "plt.plot(xgb_iterations, xgb_val_rmse_history, label=\"Validation RMSE\", color=\"orange\")\n",
        "plt.axvline(\n",
        "    xgb_bst.best_iteration + 1,\n",
        "    color=\"red\",\n",
        "    linestyle=\"--\",\n",
        "    label=f\"Best iter: {xgb_bst.best_iteration+1}\"\n",
        ")\n",
        "plt.xlabel(\"Boosting Iteration\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"XGBoost: Train vs. Validation RMSE\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# b) Feature Importance (top 20 by split count = 'weight')\n",
        "xgb_importance_dict = xgb_bst.get_score(importance_type=\"weight\")\n",
        "xgb_imp_df = (\n",
        "    pd.DataFrame.from_dict(xgb_importance_dict, orient=\"index\", columns=[\"weight\"])\n",
        "      .sort_values(\"weight\", ascending=False)\n",
        "      .head(20)\n",
        ")\n",
        "plt.figure(figsize=(6, 5))\n",
        "xgb_imp_df[\"weight\"].plot(kind=\"barh\", color=\"teal\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Number of Splits (weight)\")\n",
        "plt.title(\"XGBoost Top 20 Feature Importances\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# c) Predicted vs. Actual scatter (validation)\n",
        "xgb_true_vals = df_label_vals.squeeze()  # if df_label_vals is a one-column DataFrame, this makes it a Series\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(xgb_true_vals, xgb_val_preds, alpha=0.6, s=20, color=\"green\")\n",
        "lims = [\n",
        "    min(xgb_true_vals.min(), xgb_val_preds.min()),\n",
        "    max(xgb_true_vals.max(), xgb_val_preds.max())\n",
        "]\n",
        "plt.plot(lims, lims, linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"Actual SalePrice\")\n",
        "plt.ylabel(\"Predicted SalePrice\")\n",
        "plt.title(\"XGBoost: Predicted vs. Actual (Validation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "mCzVztT3boPN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model: Random Forest\n",
        "\n",
        "A random forest is a supervised machine learning algorithm that uses an ensemble of decision trees to make predictions. It's a powerful and versatile tool used for both classification and regression tasks."
      ],
      "metadata": {
        "id": "QVOeT94oboPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data preparation"
      ],
      "metadata": {
        "id": "xWSbhpOCboPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1a. Make copies of the raw DataFrames\n",
        "rf_X_train_raw = df_feat_trains.copy()\n",
        "rf_X_val_raw   = df_feat_vals.copy()\n",
        "\n",
        "# 1b. Replace infinities with NaN\n",
        "rf_X_train_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "rf_X_val_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 1c. Identify numerical vs. categorical columns\n",
        "rf_num_cols = rf_X_train_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "rf_cat_cols = rf_X_train_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# 1d. Build separate imputers/encoders for numeric and categorical\n",
        "rf_numeric_imputer = SimpleImputer(strategy=\"mean\")\n",
        "rf_categorical_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "rf_preprocessor = ColumnTransformer([\n",
        "    (\"num\", rf_numeric_imputer, rf_num_cols),\n",
        "    (\"cat\", rf_categorical_pipeline, rf_cat_cols)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "# 1e. Fit on training data, then transform both train and val\n",
        "rf_X_train_arr = rf_preprocessor.fit_transform(rf_X_train_raw)\n",
        "rf_X_val_arr   = rf_preprocessor.transform(rf_X_val_raw)\n",
        "\n",
        "# 1f. Get combined feature names directly:\n",
        "rf_all_names = rf_preprocessor.get_feature_names_out()\n",
        "\n",
        "rf_X_train = pd.DataFrame(\n",
        "    rf_X_train_arr,\n",
        "    index=rf_X_train_raw.index,\n",
        "    columns=rf_all_names\n",
        ")\n",
        "\n",
        "rf_X_val = pd.DataFrame(\n",
        "    rf_X_val_arr,\n",
        "    index=rf_X_val_raw.index,\n",
        "    columns=rf_all_names  # ensure same column order\n",
        ")\n",
        "\n",
        "# 1g. Handle labels (replace inf/NaN if necessary)\n",
        "rf_y_train = (df_label_trains\n",
        "              .iloc[:, 0]                        # grab the single column as a Series\n",
        "              .replace([np.inf, -np.inf], np.nan)\n",
        "              .fillna(df_label_trains.median().iloc[0])\n",
        "              .copy()\n",
        "             )\n",
        "\n",
        "rf_y_val = (df_label_vals\n",
        "            .iloc[:, 0]\n",
        "            .replace([np.inf, -np.inf], np.nan)\n",
        "            .fillna(df_label_trains.median().iloc[0])\n",
        "            .copy()\n",
        "           )"
      ],
      "metadata": {
        "trusted": true,
        "id": "YomR1UC4boPN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters Tuning"
      ],
      "metadata": {
        "id": "oUA6kJjSboPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_study = optuna.create_study(direction=\"minimize\")\n",
        "rf_study.optimize(\n",
        "    lambda trial: rf_objective(trial, rf_X_train, rf_y_train, rf_X_val, rf_y_val),\n",
        "    n_trials=50\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters found:\")\n",
        "for key, val in rf_study.best_params.items():\n",
        "    print(f\"  {key}: {val}\")"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "MW6x-aBKboPO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training"
      ],
      "metadata": {
        "id": "TJS7oZ2JboPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_best_params = rf_study.best_params.copy()\n",
        "# Ensure reproducibility and parallelism\n",
        "rf_best_params.update({\n",
        "    \"random_state\": 42,\n",
        "    \"n_jobs\": -1\n",
        "})\n",
        "\n",
        "rf_model = RandomForestRegressor(**rf_best_params)\n",
        "rf_model.fit(rf_X_train, rf_y_train)\n",
        "\n",
        "rf_val_preds   = rf_model.predict(rf_X_val)\n",
        "rf_mse         = mean_squared_error(rf_y_val, rf_val_preds, squared=True)    # Mean Squared Error\n",
        "rf_rmse        = mean_squared_error(rf_y_val, rf_val_preds, squared=False)   # Root MSE\n",
        "rf_mape        = mean_absolute_percentage_error(rf_y_val, rf_val_preds)\n",
        "rf_accuracy_pct = (1.0 - rf_mape) * 100.0\n",
        "\n",
        "print(f\"[Random Forest] Validation MSE:  {rf_mse:.4f}\")\n",
        "print(f\"[Random Forest] Validation RMSE: {rf_rmse:.4f}\")\n",
        "print(f\"[Random Forest] Validation MAPE: {rf_mape:.4f}\")\n",
        "print(f\"[Random Forest] Validation Accuracy (100 - MAPE): {rf_accuracy_pct:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "DiY0kD5gboPO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "6vMy4MwMboPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Feature importance (top 20)\n",
        "importances = rf_model.feature_importances_\n",
        "rf_imp_df = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": rf_X_train.columns,\n",
        "        \"importance\": importances\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .head(20)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.barh(rf_imp_df[\"feature\"][::-1], rf_imp_df[\"importance\"][::-1], color=\"teal\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.title(\"Random Forest Top 20 Feature Importances\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# b) Predicted vs Actual scatter (validation)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(rf_y_val.values, rf_val_preds, alpha=0.6, s=20, color=\"green\")\n",
        "lims = [\n",
        "    min(rf_y_val.min(), rf_val_preds.min()),\n",
        "    max(rf_y_val.max(), rf_val_preds.max())\n",
        "]\n",
        "plt.plot(lims, lims, linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"Actual SalePrice\")\n",
        "plt.ylabel(\"Predicted SalePrice\")\n",
        "plt.title(\"Random Forest: Predicted vs. Actual (Validation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "UQlHXePvboPR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model: CatBoost\n",
        "\n",
        "CatBoost is an open-source software library developed by Yandex. It provides a gradient boosting framework which, among other features, attempts to solve for categorical features using a permutation-driven alternative to the classical algorithm."
      ],
      "metadata": {
        "id": "jKWVsjjfboPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "pr3SxHsmboPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make copies so we don’t mutate originals\n",
        "cb_X_train_raw = df_feat_trains.copy()\n",
        "cb_X_val_raw   = df_feat_vals.copy()\n",
        "\n",
        "# 2. Replace infinities with NaN (numbers only) — can leave categorical NaN for now\n",
        "cb_X_train_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "cb_X_val_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 3. Identify categorical columns\n",
        "cb_cat_cols = cb_X_train_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# 4. Fill NaN in categorical columns with a placeholder string BEFORE casting to category\n",
        "for col in cb_cat_cols:\n",
        "    # Fill missing values with \"missing\" (or any string not already used)\n",
        "    cb_X_train_raw[col] = cb_X_train_raw[col].fillna(\"missing\").astype(\"category\")\n",
        "    cb_X_val_raw[col]   = cb_X_val_raw[col].fillna(\"missing\").astype(\"category\")\n",
        "\n",
        "# 5. Now handle labels as before (numeric labels can still have NaN/infinity imputed, etc.)\n",
        "cb_y_train = (\n",
        "    df_label_trains\n",
        "    .iloc[:, 0]\n",
        "    .replace([np.inf, -np.inf], np.nan)\n",
        "    .fillna(df_label_trains.iloc[:, 0].median())\n",
        "    .copy()\n",
        ")\n",
        "cb_y_val = (\n",
        "    df_label_vals\n",
        "    .iloc[:, 0]\n",
        "    .replace([np.inf, -np.inf], np.nan)\n",
        "    .fillna(df_label_trains.iloc[:, 0].median())\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "# 6. Build CatBoost Pools\n",
        "cb_train_pool = Pool(data=cb_X_train_raw, label=cb_y_train, cat_features=cb_cat_cols)\n",
        "cb_val_pool   = Pool(data=cb_X_val_raw,   label=cb_y_val,   cat_features=cb_cat_cols)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kUweEQcLboPS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters Tuning"
      ],
      "metadata": {
        "id": "vh7LCaf7boPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cb_study = optuna.create_study(direction=\"minimize\")\n",
        "cb_study.optimize(\n",
        "    lambda trial: cb_objective(trial, cb_train_pool, cb_val_pool),\n",
        "    n_trials=50\n",
        ")\n",
        "\n",
        "print(\"Best CatBoost params:\")\n",
        "for key, val in cb_study.best_params.items():\n",
        "    print(f\"  {key}: {val}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "HNJSw-lfboPS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "AXJUbKxUboPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cb_best_params = cb_study.best_params.copy()\n",
        "cb_best_params.update({\n",
        "    \"random_seed\": 42,\n",
        "    \"verbose\": 0\n",
        "})\n",
        "\n",
        "cb_model = CatBoostRegressor(**cb_best_params)\n",
        "cb_model.fit(\n",
        "    cb_train_pool,\n",
        "    eval_set=cb_val_pool,\n",
        "    early_stopping_rounds=50,\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "cb_val_preds = cb_model.predict(cb_val_pool)\n",
        "cb_rmse = mean_squared_error(cb_y_val, cb_val_preds, squared=False)\n",
        "cb_mape = mean_absolute_percentage_error(cb_y_val, cb_val_preds)\n",
        "cb_accuracy_pct = (1.0 - cb_mape) * 100.0\n",
        "\n",
        "print(f\"[CatBoost] Validation RMSE: {cb_rmse:.4f}\")\n",
        "print(f\"[CatBoost] Validation MAPE: {cb_mape:.4f}\")\n",
        "print(f\"[CatBoost] Validation Accuracy (100 - MAPE): {cb_accuracy_pct:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "nxwgkvQnboPS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "L_Lz2mN_boPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Feature importance (top 20)\n",
        "cb_feature_importances = cb_model.get_feature_importance(type=\"FeatureImportance\")\n",
        "cb_feat_names = cb_model.feature_names_\n",
        "cb_imp_df = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": cb_feat_names,\n",
        "        \"importance\": cb_feature_importances\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .head(20)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.barh(cb_imp_df[\"feature\"][::-1], cb_imp_df[\"importance\"][::-1], color=\"teal\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.title(\"CatBoost Top 20 Feature Importances\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# b) Predicted vs Actual scatter (validation)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(cb_y_val.values, cb_val_preds, alpha=0.6, s=20, color=\"green\")\n",
        "lims = [\n",
        "    min(cb_y_val.min(), cb_val_preds.min()),\n",
        "    max(cb_y_val.max(), cb_val_preds.max())\n",
        "]\n",
        "plt.plot(lims, lims, linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"Actual SalePrice\")\n",
        "plt.ylabel(\"Predicted SalePrice\")\n",
        "plt.title(\"CatBoost: Predicted vs. Actual (Validation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FnzTL9P8boPS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model: ElasticNet"
      ],
      "metadata": {
        "id": "KwhErXDFboPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "_FCJRfh3boPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1a. Make copies of the raw feature DataFrames so we do not mutate originals\n",
        "en_X_train_raw = df_feat_trains.copy()   # :contentReference[oaicite:1]{index=1}\n",
        "en_X_val_raw   = df_feat_vals.copy()     # :contentReference[oaicite:2]{index=2}\n",
        "\n",
        "# 1b. Replace infinities with NaN (CatBoost would handle NaN automatically, but ElasticNet needs explicit handling) :contentReference[oaicite:3]{index=3}\n",
        "en_X_train_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "en_X_val_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 1c. Identify numerical vs. categorical columns\n",
        "en_num_cols = en_X_train_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()      # :contentReference[oaicite:4]{index=4}\n",
        "en_cat_cols = en_X_train_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()    # :contentReference[oaicite:5]{index=5}\n",
        "\n",
        "# 1d. Build pipelines for numeric and categorical preprocessing:\n",
        "#     - Numeric: median imputation → standard scaling (ElasticNet benefits from standardized features) :contentReference[oaicite:6]{index=6}\n",
        "#     - Categorical: most_frequent imputation → one-hot encoding (drop first to avoid multicollinearity) :contentReference[oaicite:7]{index=7}\n",
        "en_numeric_pipeline = Pipeline([\n",
        "    (\"impute_num\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "\n",
        "en_categorical_pipeline = Pipeline([\n",
        "    (\"impute_cat\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=\"first\"))\n",
        "])\n",
        "\n",
        "# 1e. Combine them in a ColumnTransformer\n",
        "en_preprocessor = ColumnTransformer([\n",
        "    (\"num\", en_numeric_pipeline, en_num_cols),\n",
        "    (\"cat\", en_categorical_pipeline, en_cat_cols)\n",
        "], remainder=\"drop\")  # drop any other columns not specified\n",
        "\n",
        "# 1f. Fit/transform as before\n",
        "en_X_train_arr = en_preprocessor.fit_transform(en_X_train_raw)\n",
        "en_X_val_arr   = en_preprocessor.transform(en_X_val_raw)\n",
        "\n",
        "# 1g. Obtain all output feature names in one shot\n",
        "en_feature_names = en_preprocessor.get_feature_names_out()\n",
        "\n",
        "# 1h. Build DataFrames\n",
        "en_X_train = pd.DataFrame(en_X_train_arr, index=en_X_train_raw.index, columns=en_feature_names)\n",
        "en_X_val   = pd.DataFrame(en_X_val_arr,   index=en_X_val_raw.index,   columns=en_feature_names)\n",
        "\n",
        "# 1i. Handle labels: replace inf/NaN if necessary, then convert to a 1D array for training :contentReference[oaicite:12]{index=12}\n",
        "en_y_train = (\n",
        "    df_label_trains\n",
        "    .iloc[:, 0]  # if it’s a one-column DataFrame, convert to Series\n",
        "    .replace([np.inf, -np.inf], np.nan)\n",
        "    .fillna(df_label_trains.iloc[:, 0].median())\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "en_y_val = (\n",
        "    df_label_vals\n",
        "    .iloc[:, 0]\n",
        "    .replace([np.inf, -np.inf], np.nan)\n",
        "    .fillna(df_label_trains.iloc[:, 0].median())\n",
        "    .copy()\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "QeW7Owi7boPS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters Tuning"
      ],
      "metadata": {
        "id": "sBafI_EFboPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and run study (minimize RMSE) :contentReference[oaicite:20]{index=20}\n",
        "en_study = optuna.create_study(direction=\"minimize\")\n",
        "en_study.optimize(\n",
        "    lambda t: en_objective(t, en_X_train.values, en_y_train.values, en_X_val.values, en_y_val.values),\n",
        "    n_trials=50\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters for ElasticNet:\")\n",
        "for key, val in en_study.best_params.items():\n",
        "    print(f\"  {key}: {val}\")"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "IufvSIdhboPS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "YSGMJFOMboPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve best params and enforce reproducibility\n",
        "en_best_params = en_study.best_params.copy()\n",
        "en_best_params.update({\n",
        "    \"fit_intercept\": True,\n",
        "    \"max_iter\": 10_000,\n",
        "    \"random_state\": 42\n",
        "})\n",
        "\n",
        "# Instantiate the final model\n",
        "en_model = ElasticNet(**en_best_params)\n",
        "\n",
        "# Fit on the preprocessed training data\n",
        "en_model.fit(en_X_train.values, en_y_train.values)\n",
        "\n",
        "# Predict on validation\n",
        "en_val_preds = en_model.predict(en_X_val.values)\n",
        "\n",
        "# Compute RMSE and MAPE\n",
        "en_val_rmse = mean_squared_error(en_y_val.values, en_val_preds, squared=False)\n",
        "en_val_mape = mean_absolute_percentage_error(en_y_val.values, en_val_preds)\n",
        "en_val_accuracy_pct = (1.0 - en_val_mape) * 100.0\n",
        "\n",
        "print(f\"[ElasticNet] Validation RMSE: {en_val_rmse:.4f}\")\n",
        "print(f\"[ElasticNet] Validation MAPE: {en_val_mape:.4f}\")\n",
        "print(f\"[ElasticNet] Validation Accuracy (100 - MAPE): {en_val_accuracy_pct:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lul9Hn91boPS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "Nh8byIw_boPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficient magnitude plot (Top 20 in absolute value)\n",
        "en_coef = en_model.coef_\n",
        "en_coef_df = pd.DataFrame({\n",
        "    \"feature\": en_feature_names,\n",
        "    \"coefficient\": en_coef\n",
        "})\n",
        "en_coef_df[\"abs_coef\"] = en_coef_df[\"coefficient\"].abs()\n",
        "en_coef_top20 = en_coef_df.sort_values(\"abs_coef\", ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(\n",
        "    en_coef_top20[\"feature\"][::-1],\n",
        "    en_coef_top20[\"coefficient\"][::-1],\n",
        "    color=\"coral\"\n",
        ")\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.title(\"ElasticNet: Top 20 Feature Coefficients (by |coef|)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Predicted vs Actual scatter (validation)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(en_y_val.values, en_val_preds, alpha=0.6, s=20, color=\"seagreen\")\n",
        "lims = [\n",
        "    min(en_y_val.min(), en_val_preds.min()),\n",
        "    max(en_y_val.max(), en_val_preds.max())\n",
        "]\n",
        "plt.plot(lims, lims, linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"Actual SalePrice\")\n",
        "plt.ylabel(\"Predicted SalePrice\")\n",
        "plt.title(\"ElasticNet: Predicted vs. Actual (Validation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "huOupDNqboPT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model: Ensemble\n",
        "\n",
        "Ensemble top 3 models with best accuracy"
      ],
      "metadata": {
        "id": "0xJdTeiZboPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "ChJJ-B_MboPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1a. Make copies of the raw DataFrames\n",
        "ensemble_X_train_raw = df_feat_trains.copy()\n",
        "ensemble_X_val_raw   = df_feat_vals.copy()\n",
        "\n",
        "# 1b. Replace infinities with NaN\n",
        "ensemble_X_train_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "ensemble_X_val_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 1c. Identify numerical vs. categorical columns\n",
        "ensemble_num_cols = ensemble_X_train_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "ensemble_cat_cols = ensemble_X_train_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# 1d. Build separate imputers/encoders for numeric and categorical\n",
        "ensemble_numeric_imputer = SimpleImputer(strategy=\"mean\")\n",
        "ensemble_categorical_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "ensemble_preprocessor = ColumnTransformer([\n",
        "    (\"num\", ensemble_numeric_imputer, ensemble_num_cols),\n",
        "    (\"cat\", ensemble_categorical_pipeline, ensemble_cat_cols)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "# 1e. Fit on training data, then transform both train and val\n",
        "ensemble_X_train_arr = ensemble_preprocessor.fit_transform(ensemble_X_train_raw)\n",
        "ensemble_X_val_arr   = ensemble_preprocessor.transform(ensemble_X_val_raw)\n",
        "\n",
        "# 1f. Get combined feature names directly:\n",
        "ensemble_all_names = ensemble_preprocessor.get_feature_names_out()\n",
        "\n",
        "ensemble_X_train = pd.DataFrame(\n",
        "    ensemble_X_train_arr,\n",
        "    index=ensemble_X_train_raw.index,\n",
        "    columns=ensemble_all_names\n",
        ")\n",
        "\n",
        "ensemble_X_val = pd.DataFrame(\n",
        "    ensemble_X_val_arr,\n",
        "    index=ensemble_X_val_raw.index,\n",
        "    columns=ensemble_all_names  # ensure same column order\n",
        ")\n",
        "\n",
        "# 1g. Handle labels (replace inf/NaN if necessary)\n",
        "ensemble_y_train = (df_label_trains\n",
        "              .iloc[:, 0]                        # grab the single column as a Series\n",
        "              .replace([np.inf, -np.inf], np.nan)\n",
        "              .fillna(df_label_trains.median().iloc[0])\n",
        "              .copy()\n",
        "             )\n",
        "\n",
        "ensemble_y_val = (df_label_vals\n",
        "            .iloc[:, 0]\n",
        "            .replace([np.inf, -np.inf], np.nan)\n",
        "            .fillna(df_label_trains.median().iloc[0])\n",
        "            .copy()\n",
        "           )\n",
        "\n",
        "print(\"ensemble_X_train_raw.shape: \", ensemble_X_train_raw.shape)\n",
        "print(\"ensemble_X_val_raw.shape: \", ensemble_X_val_raw.shape)\n",
        "print(\"ensemble_X_val.shape: \", ensemble_X_val.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VlWNEXhWboPT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find top-3 performing models"
      ],
      "metadata": {
        "id": "g1IFrBrBboPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Put the non-ensemble models into a dictionary mapping model names to accuracy\n",
        "accuracy_dict = {\n",
        "    \"ElasticNet\": en_val_accuracy_pct,\n",
        "    \"CatBoost\": cb_accuracy_pct,\n",
        "    \"RandomForest\": rf_accuracy_pct,\n",
        "    \"XGBoost\": xgb_accuracy_pct\n",
        "}\n",
        "\n",
        "# 2. Sort models by accuracy from highest to lowest\n",
        "sorted_models = sorted(accuracy_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# 3. Print in order\n",
        "print(\"Models ranked by accuracy (best → worst):\")\n",
        "for model_name, acc in sorted_models:\n",
        "    print(f\"  {model_name}: {acc:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "gWPukYIRboPT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training"
      ],
      "metadata": {
        "id": "M_9QxyUxboPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_xgb = XGBRegressor(\n",
        "    **xgb_best_params,\n",
        "    n_estimators=xgb_bst.best_iteration + 1,  # use the same number of trees\n",
        "    random_state=42,\n",
        "    tree_method=\"hist\",\n",
        ")\n",
        "\n",
        "ensemble_xgb.fit(ensemble_X_train, ensemble_y_train)\n",
        "\n",
        "ensemble_voting = VotingRegressor([\n",
        "    (\"rf\", rf_model),\n",
        "    (\"cb\",      cb_model),\n",
        "    (\"xgb\",     ensemble_xgb)\n",
        "])\n",
        "\n",
        "ensemble_voting.fit(ensemble_X_train, ensemble_y_train)\n",
        "\n",
        "ensemble_val_preds = ensemble_voting.predict(ensemble_X_val)\n",
        "\n",
        "ensemble_val_rmse = mean_squared_error(ensemble_y_val, ensemble_val_preds, squared=False)\n",
        "ensemble_val_mape = mean_absolute_percentage_error(ensemble_y_val, ensemble_val_preds)\n",
        "ensemble_val_accuracy_pct = (1.0 - ensemble_val_mape) * 100.0\n",
        "\n",
        "print(f\"[Ensemble] Validation RMSE: {ensemble_val_rmse:.4f}\")\n",
        "print(f\"[Ensemble] Validation MAPE: {ensemble_val_mape:.4f}\")\n",
        "print(f\"[Ensemble] Validation Accuracy (100 - MAPE): {ensemble_val_accuracy_pct:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "VuL5ZMfpboPT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visuals"
      ],
      "metadata": {
        "id": "Y7ih-yaOboPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Predicted vs. Actual scatter (validation) for the ensemble\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(ensemble_y_val.values, ensemble_val_preds, alpha=0.6, s=20, color=\"purple\")\n",
        "lims = [\n",
        "    min(ensemble_y_val.min(), ensemble_val_preds.min()),\n",
        "    max(ensemble_y_val.max(), ensemble_val_preds.max())\n",
        "]\n",
        "plt.plot(lims, lims, linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"Actual SalePrice\")\n",
        "plt.ylabel(\"Predicted SalePrice\")\n",
        "plt.title(\"Ensemble: Predicted vs. Actual (Validation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# b) Feature importances from the Random Forest and XGBoost components\n",
        "#    (CatBoost has its own importance, but here we display RF and XGB side by side)\n",
        "\n",
        "# RF importances (top 20)\n",
        "rf_importances = rf_model.feature_importances_\n",
        "rf_imp_df = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": ensemble_all_names,\n",
        "        \"importance\": rf_importances\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .head(20)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.barh(rf_imp_df[\"feature\"][::-1], rf_imp_df[\"importance\"][::-1], color=\"teal\")\n",
        "plt.xlabel(\"RF Importance\")\n",
        "plt.title(\"Random Forest Top 20 Feature Importances (Ensemble)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# XGB importances (top 20 by weight)\n",
        "xgb_importance_dict = ensemble_xgb.get_booster().get_score(importance_type=\"weight\")\n",
        "xgb_imp_df = (\n",
        "    pd.DataFrame.from_dict(xgb_importance_dict, orient=\"index\", columns=[\"weight\"])\n",
        "      .sort_values(\"weight\", ascending=False)\n",
        "      .head(20)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "xgb_imp_df[\"weight\"].plot(kind=\"barh\", color=\"orange\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"XGB Split Count (weight)\")\n",
        "plt.title(\"XGBoost Top 20 Feature Importances (Ensemble)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "N2hevIH6boPT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Conclusion"
      ],
      "metadata": {
        "id": "4e47i5EdboPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Put them into a dictionary mapping model names to accuracy\n",
        "accuracy_dict = {\n",
        "    \"Ensemble\": ensemble_val_accuracy_pct,\n",
        "    \"ElasticNet\": en_val_accuracy_pct,\n",
        "    \"CatBoost\": cb_accuracy_pct,\n",
        "    \"RandomForest\": rf_accuracy_pct,\n",
        "    \"XGBoost\": xgb_accuracy_pct\n",
        "}\n",
        "\n",
        "# 2. Find the model with the highest accuracy\n",
        "best_model = max(accuracy_dict, key=accuracy_dict.get)\n",
        "best_accuracy = accuracy_dict[best_model]\n",
        "\n",
        "# 3. Print the result\n",
        "print(f\"Best model: {best_model} with accuracy {best_accuracy:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "v4wn3P3cboPU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Submit the Competition\n",
        "\n",
        "Using the model with best accuracy so far"
      ],
      "metadata": {
        "id": "Ick2lSwNboPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load test.csv and split off Id\n",
        "t_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
        "t_ids = t_df[\"Id\"]\n",
        "t_X_raw = t_df.drop(\"Id\", axis=1)\n",
        "\n",
        "# 2. Replace infinities with NaN (consistent with training preprocessing)\n",
        "t_X_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# 3. Identify categorical columns for CatBoost (as during training)\n",
        "t_cat_cols = t_X_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# 4. Fill NaN in categorical columns for CatBoost and cast to category dtype\n",
        "t_X_cb = t_X_raw.copy()\n",
        "for col in t_cat_cols:\n",
        "    t_X_cb[col] = t_X_cb[col].fillna(\"missing\").astype(\"category\")\n",
        "\n",
        "# 5. Prepare test features for each model:\n",
        "# 5a. ElasticNet & RandomForest & Ensemble use ensemble_preprocessor (one-hot + impute)\n",
        "t_X_en_arr = ensemble_preprocessor.transform(t_X_raw)\n",
        "t_X_en = pd.DataFrame(\n",
        "    t_X_en_arr,\n",
        "    index=t_X_raw.index,\n",
        "    columns=ensemble_all_names\n",
        ")\n",
        "\n",
        "# 5b. RandomForest and Ensemble use the same as ensemble_preprocessor\n",
        "t_X_rf = t_X_en.copy()\n",
        "\n",
        "# 5c. XGBoost uses xgb_preprocessor (its own numeric+one-hot pipeline)\n",
        "t_X_xgb_arr = xgb_preprocessor.transform(t_X_raw)\n",
        "dtest_xgb = xgb.DMatrix(t_X_xgb_arr, feature_names=xgb_feature_names)\n",
        "\n",
        "# 5d. CatBoost Pool\n",
        "dtest_cb = Pool(data=t_X_cb, cat_features=t_cat_cols)\n",
        "\n",
        "# 6. Determine best model (must match previously computed accuracies)\n",
        "accuracy_dict = {\n",
        "    \"Ensemble\": ensemble_val_accuracy_pct,\n",
        "    \"ElasticNet\": en_val_accuracy_pct,\n",
        "    \"CatBoost\": cb_accuracy_pct,\n",
        "    \"RandomForest\": rf_accuracy_pct,\n",
        "    \"XGBoost\": xgb_accuracy_pct\n",
        "}\n",
        "best_model = max(accuracy_dict, key=accuracy_dict.get)\n",
        "\n",
        "# 7. Predict with the best model\n",
        "if best_model == \"ElasticNet\":\n",
        "    test_preds = en_model.predict(t_X_en.values)\n",
        "\n",
        "elif best_model == \"RandomForest\":\n",
        "    test_preds = rf_model.predict(t_X_rf.values)\n",
        "\n",
        "elif best_model == \"XGBoost\":\n",
        "    test_preds = xgb_bst.predict(dtest_xgb, iteration_range=(0, xgb_bst.best_iteration))\n",
        "\n",
        "elif best_model == \"CatBoost\":\n",
        "    test_preds = cb_model.predict(dtest_cb)\n",
        "\n",
        "elif best_model == \"Ensemble\":\n",
        "    # Ensemble expects the same preprocessing as RandomForest/ElasticNet\n",
        "    test_preds = ensemble_voting.predict(t_X_en)\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown best_model: {best_model}\")\n",
        "\n",
        "# 8. Build submission DataFrame and save\n",
        "submission_df = pd.DataFrame({\"Id\": t_ids, \"SalePrice\": test_preds})\n",
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SNupBTMPboPU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Roadmap\n",
        "\n",
        "Potential improvements for future iteration:\n",
        "1.  Feed all features into the models and compare the result with the current approach.\n",
        "2.  Attempt to selectively remove the features based on each model's feature importance"
      ],
      "metadata": {
        "id": "VXDD79eiboPU"
      }
    }
  ]
}